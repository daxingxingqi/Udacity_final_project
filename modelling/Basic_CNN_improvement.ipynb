{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import SpatialDropout1D, Dense, Flatten, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from IPython.display import display\n",
    "import itertools\n",
    "from nltk.corpus import words\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('./data/cleaned_train.csv')\n",
    "test = pd.read_csv('./data/cleaned_test.csv')\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_NaN_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_NaN_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "embed_size   = 300    # how big is each word vector\n",
    "max_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen       = 400   # max number of words in a comment to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sentences and convert to integers\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/glove.840B.300d.txt')\n",
    "embeddings_index = {}\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec, Layer\n",
    "import tensorflow as tf\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)#Please switch to tf.metrics.auc. Note that the order of the labels and predictions arguments has been switched.\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "conv_filters = 180 # No. filters to use for each convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "inp = Input(shape=(X_train.shape[1],), dtype='int32')\n",
    "emb = Embedding(max_features, embed_size, weights=[embedding_matrix], input_length = maxlen, trainable = False)(inp)\n",
    "emb = SpatialDropout1D(0.2)(emb)\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=1, kernel_initializer = 'normal')(emb)\n",
    "actv1_1 = Activation('relu')(conv1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=2, kernel_initializer = 'normal')(emb)\n",
    "actv1_2 = Activation('relu')(conv1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=3, kernel_initializer = 'normal')(emb)\n",
    "actv1_3 = Activation('relu')(conv1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "conv1_4 = Conv1D(filters=conv_filters, kernel_size=4, kernel_initializer = 'normal')(emb)\n",
    "actv1_4 = Activation('relu')(conv1_4)\n",
    "glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "drp1 = Dropout(0.6)(cnct)\n",
    "dns1  = Dense(144, activation='relu')(drp1)\n",
    "out = Dense(y.shape[1],kernel_constraint=max_norm(3.), activation='sigmoid')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "adam = Adam(lr=1e-3, decay=1e-7)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "             ModelCheckpoint(filepath='./models/best_CNN_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/50\n",
      "143613/143613 [==============================] - 51s 355us/step - loss: 0.0706 - acc: 0.9765 - val_loss: 0.0547 - val_acc: 0.9789\n",
      "Epoch 2/50\n",
      "143613/143613 [==============================] - 50s 346us/step - loss: 0.0506 - acc: 0.9808 - val_loss: 0.0470 - val_acc: 0.9819\n",
      "Epoch 3/50\n",
      "143613/143613 [==============================] - 50s 347us/step - loss: 0.0476 - acc: 0.9818 - val_loss: 0.0448 - val_acc: 0.9823\n",
      "Epoch 4/50\n",
      "143613/143613 [==============================] - 50s 348us/step - loss: 0.0456 - acc: 0.9822 - val_loss: 0.0438 - val_acc: 0.9828\n",
      "Epoch 5/50\n",
      "143613/143613 [==============================] - 50s 349us/step - loss: 0.0440 - acc: 0.9828 - val_loss: 0.0431 - val_acc: 0.9830\n",
      "Epoch 6/50\n",
      "143613/143613 [==============================] - 50s 349us/step - loss: 0.0432 - acc: 0.9829 - val_loss: 0.0426 - val_acc: 0.9834\n",
      "Epoch 7/50\n",
      "143613/143613 [==============================] - 50s 349us/step - loss: 0.0423 - acc: 0.9833 - val_loss: 0.0426 - val_acc: 0.9831\n",
      "Epoch 8/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0410 - acc: 0.9836 - val_loss: 0.0423 - val_acc: 0.9833\n",
      "Epoch 9/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0403 - acc: 0.9839 - val_loss: 0.0433 - val_acc: 0.9826\n",
      "Epoch 10/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0394 - acc: 0.9842 - val_loss: 0.0428 - val_acc: 0.9836\n",
      "Epoch 11/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0387 - acc: 0.9846 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 12/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 13/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0375 - acc: 0.9849 - val_loss: 0.0427 - val_acc: 0.9834\n",
      "Epoch 14/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0369 - acc: 0.9850 - val_loss: 0.0423 - val_acc: 0.9831\n",
      "Epoch 15/50\n",
      "143613/143613 [==============================] - 50s 351us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0437 - val_acc: 0.9828\n",
      "Epoch 16/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0361 - acc: 0.9855 - val_loss: 0.0427 - val_acc: 0.9833\n",
      "Epoch 17/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0354 - acc: 0.9857 - val_loss: 0.0427 - val_acc: 0.9833\n",
      "Epoch 18/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0427 - val_acc: 0.9836\n",
      "Epoch 19/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0435 - val_acc: 0.9832\n",
      "Epoch 20/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0433 - val_acc: 0.9830\n",
      "Epoch 21/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0338 - acc: 0.9862 - val_loss: 0.0433 - val_acc: 0.9831\n",
      "Epoch 22/50\n",
      "143613/143613 [==============================] - 51s 352us/step - loss: 0.0334 - acc: 0.9862 - val_loss: 0.0440 - val_acc: 0.9830\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = model.fit(X_train, # Features\n",
    "                      y, # Target vector\n",
    "                      epochs=50, # Number of epochs\n",
    "                      callbacks=callbacks, # Early stopping\n",
    "                      verbose=1, # Print description after each epoch\n",
    "                      batch_size=256, # Number of observations per batch\n",
    "                      validation_split=0.1,\n",
    "                      shuffle=True) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./models/best_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submid = pd.DataFrame({'id': test[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = list_classes)], axis=1)\n",
    "submission.to_csv('conv_glove_simple_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
