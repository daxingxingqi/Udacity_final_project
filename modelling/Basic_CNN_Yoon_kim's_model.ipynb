{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2b7f058-8cf5-4599-b099-f58a17b27f91",
    "_uuid": "7b486ed1856a061e75f27c240c22886c565efa63"
   },
   "source": [
    "## Keras implementation of Yoon Kim's model for sentence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2b62e7b-a49b-4501-b303-ef7328fa208d",
    "_uuid": "71345317674baa7f0ab5e029cb2a07df9262fe6b"
   },
   "source": [
    "##### The following is a keras implementation of Yoon Kim's convolutional neural network model for sentence classification from the paper: https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "44eb8bb2-979b-4dc9-8f36-0c6a7a74823a",
    "_uuid": "5d4c457e9946e88f79fe3efd29cf601ebe9987db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from IPython.display import display\n",
    "import itertools\n",
    "from nltk.corpus import words\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a37ed914-3787-4bbe-847c-3ba7a3c93b39",
    "_uuid": "7394571847983f631cf44419e010532f1e7a525c"
   },
   "source": [
    "##### Determine dimension of embedding vector, max size of vocabulary and max length of sentence (crop the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "76e9a099-026b-4632-b0ff-29329df05499",
    "_uuid": "c21b046383b3dcdcdd8319a3ba4f51896a5ac385"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "embed_size   = 50    # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen       = 100   # max number of words in a comment to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cbda94d6-ebdf-46fa-824d-22c23dbbaf36",
    "_uuid": "0ccdb2895aa8e190f9f427290912282f6cb6baea"
   },
   "source": [
    "##### Load data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "f66337db-af9a-4604-b675-f8c229084e85",
    "_uuid": "dca42dbb8cad2faa1b4527dedaa031c8939aa523"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train=pd.read_csv(\"./data/train.csv\")\n",
    "test=pd.read_csv(\"./data/test.csv\")\n",
    "X_train = train[\"comment_text\"]\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "X_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qizichen1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE,'',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)# delete stopwords from text\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = []\n",
    "list_sentences_test = []\n",
    "for text in X_train:\n",
    "    list_sentences_train.append(text_prepare(text))\n",
    "    \n",
    "for text in X_test:\n",
    "    list_sentences_test.append(text_prepare(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7e84b2b-0b6b-4d26-96de-93714744992b",
    "_uuid": "214b0c3803ce23163787c559bf8c103271cacc9b"
   },
   "source": [
    "##### Tokenize sentences, convert to integers and pad sentences < 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "09b529ed-38f0-44db-a97d-ca725d1c1f4f",
    "_uuid": "06ec3d6800e124b935178c6b981cfc5e27a3c7ad"
   },
   "outputs": [],
   "source": [
    "# Pad sentences and convert to integers\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "901cbce3-4e2d-48a5-80de-48366ced45a4",
    "_uuid": "699edb3012216da4bf9754b706da58fc4fd10912"
   },
   "source": [
    "##### Load \"glove\" pre-trained embeddings and construct vocabulary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8a8fccae-6501-432f-8db4-2ae4e7847c40",
    "_uuid": "0ba1b991235f978f3648532f2a4a93af2ec0bfd3"
   },
   "outputs": [],
   "source": [
    "# Read the glove word vectors (space delimited strings) into a dictionary from word->vector\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('./data/glove.6B/glove.6B.50d.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce1655d7-b5dd-4f48-a3ef-e4b02bb5159f",
    "_uuid": "febf2cd76cf3a6a6895a331c8e900f9df382bbb2"
   },
   "source": [
    "##### Create embedding matrix and initialize space for new words not present in \"glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c4c3a7b3-6ad3-4cc3-acc4-8aeb18b30856",
    "_uuid": "8d96bbe653615bca5b5e0239e7a0adc337b69085"
   },
   "outputs": [],
   "source": [
    "# Create embeddings matrix\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# Create embedding matrix using our vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# Initialize embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "# Loop through each word and get its embedding vector\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "        continue # Skip words appearing less than the minimum allowed\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "71d0d9ae-0024-418d-92aa-8d91f10c78e3",
    "_uuid": "e9999f863832b0790840cdc5cc1296eca67c41bd"
   },
   "source": [
    "##### Set no. of convolution filters and weigh the outcome variable in order to balance.\n",
    "- 128 filters are used for each convolution. I.e. with a kernel size of 3, 128 tri grams are constructed each representing a specific feature. With a kernel size of 4, 128 4-grams are constructed and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "38d9b786-e556-4e92-ba9c-7b47bab9bc58",
    "_uuid": "53ed470e95c215e0ecc5be9df9f66402fb08726b"
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "conv_filters = 100 # No. filters to use for each convolution\n",
    "weight_vec = list(np.max(np.sum(y, axis=0))/np.sum(y, axis=0))\n",
    "class_weight = {i: weight_vec[i] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 9.58871473354232,\n",
       " 2: 1.8101550479346669,\n",
       " 3: 31.99581589958159,\n",
       " 4: 1.941602132791672,\n",
       " 5: 10.885409252669039}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "910d419d-7767-40fd-af9f-085e7e687b12",
    "_uuid": "570facca18c7ca89ef615c47ce562811b620c431"
   },
   "source": [
    "##### Construct Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_cell_guid": "eaf137d0-229e-4130-a938-e30b7c8f8f66",
    "_uuid": "bc4b69a2db4bf1e4262d6aec4e7671d9353f084c"
   },
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "inp = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "emb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "emb = Dropout(0.5)(emb)\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=1)(emb)\n",
    "actv1_1 = Activation('relu')(conv1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=2)(emb)\n",
    "actv1_2 = Activation('relu')(conv1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n",
    "actv1_3 = Activation('relu')(conv1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "conv1_4 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n",
    "actv1_4 = Activation('relu')(conv1_4)\n",
    "glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "drp1 = Dropout(0.5)(cnct)\n",
    "dns1  = Dense(32, activation='relu')(drp1)\n",
    "out = Dense(y.shape[1],kernel_constraint=max_norm(3.), activation='sigmoid')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 100, 50)      1000000     input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 100, 50)      0           embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 100, 100)     5100        dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 99, 100)      10100       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 98, 100)      15100       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 97, 100)      20100       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 100, 100)     0           conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 99, 100)      0           conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 98, 100)      0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 97, 100)      0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_73 (Global (None, 100)          0           activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_74 (Global (None, 100)          0           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_75 (Global (None, 100)          0           activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_76 (Global (None, 100)          0           activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 400)          0           global_max_pooling1d_73[0][0]    \n",
      "                                                                 global_max_pooling1d_74[0][0]    \n",
      "                                                                 global_max_pooling1d_75[0][0]    \n",
      "                                                                 global_max_pooling1d_76[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 400)          0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 32)           12832       dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 6)            198         dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,063,430\n",
      "Trainable params: 1,063,430\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model(inputs=inp, outputs=out).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_cell_guid": "dd176424-156f-4e7e-8160-b3cae7a1a342",
    "_uuid": "1bfa4501e43e8b0784b7bafcac9e2d47a5043d9c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_cell_guid": "18960cdb-ab02-4052-8f05-0c794047c8c6",
    "_uuid": "7d8a9ad0b0b2b5aafb6d697a78380a53e3efe77e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 170s 1ms/step - loss: 0.0944 - acc: 0.9733 - val_loss: 0.1056 - val_acc: 0.9723\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 173s 1ms/step - loss: 0.0711 - acc: 0.9786 - val_loss: 0.0796 - val_acc: 0.9793\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 175s 1ms/step - loss: 0.0657 - acc: 0.9797 - val_loss: 0.0750 - val_acc: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7fd1ac18>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate model\n",
    "model.fit(X_train, y, validation_split=0.1, epochs=3, batch_size=50, shuffle=True, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e1d85ed-c594-4134-a5e8-00f046dbb1ab",
    "_uuid": "f744a0429fd2706808016b917f2194ac5bbe6f84"
   },
   "source": [
    "##### Predict and finally submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_cell_guid": "82c412ed-e24d-46bf-9912-99c255eb1ca9",
    "_uuid": "a445e8f05353f391ab56ab5f50248e6417751190"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_cell_guid": "628ae9f9-5dee-4b74-bdcd-b1e2a16c2fbb",
    "_uuid": "b6e0af2175dccc76623c108305592ac209780e51"
   },
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submid = pd.DataFrame({'id': test[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = list_classes)], axis=1)\n",
    "submission.to_csv('conv_glove_simple_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
